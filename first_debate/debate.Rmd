---
title: "Sentiment Analysis: First Presidential Debate 2016"
author: "Yanfei Wu"
date: "September 29, 2016"
output: 
  html_document: 
    highlight: pygments
    theme: spacelab
    keep_md: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r packages, include = F}
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(plyr)
library(dplyr)
library(stringr)
library(quanteda)
library(wordcloud)
```  

```{r set theme, include = T}
theme_set(theme_bw())
```

## Introduction  

The US presidential election of 2016 is in less than 2 months. After series of presidential primary elections and caucuses, businessman Donald Trump became the Republican Party's presidential nominee and former Secretary of State Hillary Clinton became the Democratic Party's presidential nominee. Before November, there are three presidential debates between Clinton and Trump. The first debate just took place several days ago on September 26th.    

In this post, I show some results of the sentiment analysis I did using tweets collected after the first debate. All codes for this post can be found in [my Github Repository](https://github.com/yanfei-wu/).   

## Getting Data From Twitter   

```{r read.data}
# the scraping code is in the seperate R file

clinton.txt <- readRDS('clinton_tweet_cleaned.RData')
trump.txt <- readRDS('trump_tweet_cleaned.RData')

# merge the tweets
tweet <- c(clinton.txt, trump.txt)
```

20,000 tweets were quried using the two candidates' names (10,000 tweets for each candidate) from Twitter. An additional filtering step was carried out to remove tweets containing both names just to simplify the assignment of the sentiment scores to each candidate. (`r length(clinton.txt)` tweets were left only mentioning Clinton's name and `r length(trump.txt)` tweets only mentioning Trump's name.)    

## Sentiment Analysis  

The sentiment scores were calculated using a [lexicon-based method](https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf) as proposed by Hu and Liu. A list of English positive and negative opinion words or sentiment words compiled by them were used (you can find it [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)). The different between the number of positive words and the number of negative words in each tweet was used to determine the option orientation or the sentiment score of each tweet.   

*Note*: Before calculating the sentiment scores, the tweets were cleaned by removing punctuations, special characters (@ and #), and URLs.  

```{r sentiment.score, include = FALSE}

# adapted from: 
# https://jeffreybreen.wordpress.com/2011/07/04/twitter-text-mining-r-slides/
score.sentiment <- function(sentences, pos.words, neg.words, .progress = 'none') { 

    require(plyr)
    require(stringr)
    
    # use laply to return scores from a vector of sentences
    scores <- laply(sentences, function(sentence, pos.words, neg.words) {
        
        # clean up sentences
        sentence <- gsub('[[:punct:]]', '', sentence)
        sentence <- gsub('[[:cntrl:]]', '', sentence)
        sentence <- gsub('\\d+', '', sentence)
        sentence <- gsub('http\\S+\\s*', '', sentence)
        sentence <- gsub('#\\w+ *', '', sentence)
        sentence <- gsub('@\\w+ *', '', sentence)
        
        # split into words
        word.list <- str_split(sentence, '\\s+')
        words <- unlist(word.list)
        
        # compare words to the dictionaries of positive & negative terms
        pos.matches <- match(words, pos.words)
        neg.matches <- match(words, neg.words)
        
        # find non-NA values, i.e., matches
        pos.matches <- !is.na(pos.matches)
        neg.matches <- !is.na(neg.matches)
        
        #  Score  =  Number of positive words  -  Number of negative words
        score <- sum(pos.matches) - sum(neg.matches)
        
        return(score)
    }, pos.words, neg.words, .progress = .progress )
    
    scores.df <- data.frame(score = scores, tweet = sentences)
    return(scores.df)
}
```  

```{r score.df}
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

tweet.num <- c(length(clinton.txt), length(trump.txt))

scores <- score.sentiment(tweet, pos, neg, .progress = 'text')
scores$name <- factor(rep(c('Clinton', 'Trump'), tweet.num))

scores$sentiment <- 0
scores$sentiment[scores$score <= -2] <- -1
scores$sentiment[scores$score >= 2] <- 1 
```  

Now, let's look at the distribution of sentiment scores for each candidate.    

```{r histogram, fig.width = 4.5, fig.height = 6, fig.align = 'center'}
ggplot(scores, aes(x = score)) + 
    geom_histogram(fill = 'blue', color = 'black', bins = 15) +
    facet_grid(name ~.) + 
    labs(x = 'Scores', y = 'Count', 
         title = 'Sentiment Scores Histogram Comparison')
```

```{r boxplot, fig.width = 5, fig.height = 4, fig.align = 'center'}
ggplot(scores, aes(x = name, y = score)) + 
    geom_boxplot(aes(fill = name)) + 
    labs(x = '', y = 'Sentiment Scores', 
         title = 'Sentiment Scores Boxplot Comparison') +
    theme(legend.title = element_blank())
```

Both candidates have similar variance in terms of their sentiment score distributions with a similar median roughly around 0. And both have outliers or extreme scores (positive and negative). Overall, it is interesting that based on these tweets **people seem to be less happy with Clinton than with Trump**.   

The raw sentiment scores were then used to divide the sentiment into 3 categoris below:  

* Positive(1): sentiment score >= 2  
* Neutral(0): -2 < sentiment score < 2  
* Negative(-1): sentiment score <= -2  

The distributions of positive, neutral, and negative sentiments for each candidate are shown below:

```{r barplot, fig.width = 5, fig.height = 8, fig.align = 'center'}
plot1 <- ggplot(scores[scores$name == 'Clinton', ], aes(x = factor(sentiment))) + 
    geom_bar(aes(y = (..count..)/sum(..count..), fill = factor(sentiment))) + 
    labs(x = 'Sentiment', y = 'Frequency', title = 'Clinton') + 
    scale_fill_discrete(name = "Sentiment",
                        breaks = c(-1, 0, 1),
                        labels=c("Negative", "Neutral", "Positive"))

plot2 <- ggplot(scores[scores$name == 'Trump', ], aes(x = factor(sentiment))) + 
    geom_bar(aes(y = (..count..)/sum(..count..), fill = factor(sentiment))) +
    labs(x = 'Sentiment', y = 'Frequency', title = 'Trump') + 
    scale_fill_discrete(name = "Sentiment",
                        breaks = c(-1, 0, 1),
                        labels=c("Negative", "Neutral", "Positive"))

grid.arrange(plot1, plot2, nrow = 2)
```

Again, the majority of tweets are neutual towards both candidates but there are indeed bias! **More tweets are negative about Clinton than those that are positive**.      


## Word Frequency Analysis  

Another interesting thing we can do with tweets is to a word frequency analysis. The first step is to construct corpus by compling the tweets. Then, the texts in the corpus are broken into tokens, i.e., single word. After tokenization, document-term matrix is created to describe the frequency of each word that occurs in the corpus. Note that some words are removed from this analysis such as the candidates' names and English stopwords.  


```{r word.frequency.corpus, include = FALSE}
# clean the text
clean <- function(sentences) { 
    require(plyr)
    clean.text <- laply(sentences, function(sentence) {
        sentence <- gsub('http\\S+\\s*', '', sentence)
        sentence <- gsub('#\\w+ *', '', sentence)
        sentence <- gsub('@\\w+ *', '', sentence)
        sentence <- gsub('[[:cntrl:]]', '', sentence)
        sentence <- gsub('[[:punct:]]', '', sentence)
        sentence <- gsub('\\d+', '', sentence)
        sentence <- gsub('rt', '', sentence)
        return(sentence)
    })
    return(clean.text)
}

clinton.txt <- clean(clinton.txt)
trump.txt <- clean(trump.txt)

# construct corpus
clinton.corpus <- corpus(clinton.txt)
trump.corpus <- corpus(trump.txt)

# define stopwords
stop.words <- c(stopwords('english'), 'clinton', 'hillary', 'donald', 'trump', 'dont',
                'usa', 'us', 'editorial', 'clintons', 'trumps', 'will', 'now', 'just',
                'still', 'can', 'via', 'new', 'says', 'today', 'amp', 'see', 'time', 'ht')

# tokenization and dfm
clinton.token <- tokenize(clinton.corpus, ngrams = 1, verbose = F)
clinton.dfm <- dfm(clinton.token, ignoredFeatures = stop.words)
trump.token <- tokenize(trump.corpus, ngrams = 1, verbose = F)
trump.dfm <- dfm(trump.token, ignoredFeatures = stop.words)
```

The frequency plots are shown below:  

```{r top.words}
clinton.top <- data.frame(word = rownames(as.matrix(topfeatures(clinton.dfm, 25))), 
                      freq = as.matrix(topfeatures(clinton.dfm, 25))[, 1])

trump.top <- data.frame(word = rownames(as.matrix(topfeatures(trump.dfm, 25))), 
                      freq = as.matrix(topfeatures(trump.dfm, 25))[, 1])
``` 

```{r word.frequency.plot, fig.width = 9, fig.height = 6, fig.align = 'center'}
plot1 <- ggplot(clinton.top, aes(x = word, y = freq)) + 
    geom_bar(stat = "identity", fill = 'blue') + 
    labs(x = '', y = 'Frequency', title = 'Top 25 Words in Tweets about Clinton') + 
    coord_flip() + scale_x_discrete(limits = clinton.top$word)

plot2 <- ggplot(trump.top, aes(x = word, y = freq)) + 
    geom_bar(stat = "identity", fill = 'blue') + 
    labs(x = '', y = 'Frequency', title = 'Top 25 Words in Tweets about Trump') + 
    coord_flip() + scale_x_discrete(limits = trump.top$word)

grid.arrange(plot1, plot2, ncol = 2)
```

```{r word.cloud.clinton, fig.width = 5, fig.height = 5, fig.align = 'center'}
plot(clinton.dfm, max.words = 50, scale = c(3, .2))
title('Word Cloud from Tweets about Clinton')
```

```{r word.cloud.trump, fig.width = 5, fig.height = 5, fig.align = 'center'}
plot(trump.dfm, max.words = 50, scale = c(3, .2))
title('Word Cloud from Tweets about Trump')
```

Not surprisingly, words like 'Bill', 'Emails', 'FBI' appear in tweets about Clinton quite frequently. It also looks like a lot of tweets about Clinton also mention Gary Johnson. In tweets about Trump, some frequent words are 'race', 'Obama', and also words like 'unfit'!   


## Summary  

Tweets about the two presidential candidates, Donald Trump and Hillary Clinton, after the first presidential debate were used for sentiment analysis. Sentiment scores, defined as the difference between the number of positive words and that of the negative words based on the analyzed tweets shows a more positive sentiment towards Trump. The majority of the tweets, however, are neutral. The word frequency analysis shows frequent words like 'Bill', 'Emails' appearing in tweets about Clinton, and words like 'race', 'Obama' frequently showing up in tweets about Trump. 

